{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msmic\\Documents\\code\\rl\\carla\\rl_ad\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym \n",
    "\n",
    "from decision_transformer.models.encoder import get_encoder\n",
    "from decision_transformer.envs.carracing import CarRacing\n",
    "from decision_transformer.models.gcsl_idm_org import GCSL_IDM\n",
    "# dotdict is a dictionary that allows to access its keys as attributes\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'device': 'cuda',\n",
    "    'K': 20,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['observations', 'actions', 'rewards', 'dones'])\n"
     ]
    }
   ],
   "source": [
    "# load data/carracing-medium-v2.pkl\n",
    "with open('data/carracing-medium-v2.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "    print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mean = np.mean([np.mean(traj['observations'], axis=0) for traj in data.values()], axis=0)\n",
    "state_std = np.mean([np.std(traj['observations'], axis=0) for traj in data.values()], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mean_cp, state_std_cp = np.load('carracing_v1_ME_state_mean_std.npz').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((763, 64), (763, 2), (763,), (763,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['observations'].shape, data[0]['actions'].shape, data[0]['rewards'].shape, data[0]['dones'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_tanh=False):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.state_embed = nn.Linear(input_dim, 128)\n",
    "        self.reward_embed = nn.Linear(1, 128)\n",
    "        self.use_tanh = use_tanh\n",
    "\n",
    "        self.predict_state = nn.Sequential(nn.Linear(256, 400),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(400, 300),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(300, output_dim))\n",
    "\n",
    "    def forward(self, state, reward):\n",
    "        state = self.state_embed(state)\n",
    "        reward = self.reward_embed(reward.unsqueeze(1))\n",
    "        state_reward = torch.cat((state, reward), dim=1)\n",
    "        if self.use_tanh:\n",
    "            return torch.tanh(self.predict_state(state_reward))\n",
    "        return self.predict_state(state_reward)\n",
    "    \n",
    "    def get_action(self, states, actions, rewards, target_returns, dones):\n",
    "        print(states.shape, actions.shape, rewards.shape, target_returns.shape, dones.shape)\n",
    "        state = states[-1].unsqueeze(0)\n",
    "        reward = target_returns[:,-1].unsqueeze(0)\n",
    "        print(state.shape, reward.shape)\n",
    "        state = self.state_embed(state)\n",
    "        reward = self.reward_embed(reward)\n",
    "        state_reward = torch.cat((state, reward), dim=1)\n",
    "        return self.predict_state(state_reward)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_predictor = Predictor(64, 64)\n",
    "state_predictor.load_state_dict(torch.load('state_predictor_carracer.pt'))\n",
    "action_predictor = Predictor(64, 2, use_tanh=True)\n",
    "action_predictor.load_state_dict(torch.load('action_predictor_carracer.pt'))\n",
    "encoder = get_encoder('decision_transformer/models/conv_net_500_v10.pt')\n",
    "\n",
    "gcsl_predictor = GCSL_IDM(64, 2, 1000, args)\n",
    "gcsl_predictor.load_state_dict(torch.load('tmp\\\\carracing\\\\idm_gcsl_gcsl+\\\\2023_05_12_16_29_53_291165\\\\tensorboard\\\\iter_100.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode_rtg(\n",
    "        env,\n",
    "        state_dim,\n",
    "        act_dim,\n",
    "        model,\n",
    "        max_ep_len=1000,\n",
    "        scale=1000.,\n",
    "        state_mean=0.,\n",
    "        state_std=1.,\n",
    "        device='cuda',\n",
    "        encoder=None,\n",
    "        target_return=None,\n",
    "        mode='normal',\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device=device)\n",
    "\n",
    "    state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "    state_std = torch.from_numpy(state_std).to(device=device)\n",
    "\n",
    "    state = env.reset()\n",
    "    if encoder is not None:\n",
    "        state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        state = encoder(state).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    if mode == 'noise':\n",
    "        state = state + np.random.normal(0, 0.1, size=state.shape)\n",
    "\n",
    "    # we keep all the histories on the device\n",
    "    # note that the latest action and reward will be \"padding\"\n",
    "    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "    ep_return = target_return\n",
    "    target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "\n",
    "    sim_states = []\n",
    "\n",
    "    episode_return, episode_length = 0, 0\n",
    "    for t in range(max_ep_len):\n",
    "\n",
    "        # add padding\n",
    "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "        action = model.get_action(\n",
    "            (states.to(dtype=torch.float32) - state_mean) / state_std,\n",
    "            actions.to(dtype=torch.float32),\n",
    "            rewards.to(dtype=torch.float32),\n",
    "            target_return.to(dtype=torch.float32),\n",
    "            timesteps.to(dtype=torch.long),\n",
    "        )\n",
    "        actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if encoder is not None:\n",
    "            state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "            state = encoder(state).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "        states = torch.cat([states, cur_state], dim=0)\n",
    "        rewards[-1] = reward\n",
    "\n",
    "        if mode != 'delayed':\n",
    "            pred_return = target_return[0,-1] - (reward/scale)\n",
    "        else:\n",
    "            pred_return = target_return[0,-1]\n",
    "        target_return = torch.cat(\n",
    "            [target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "        timesteps = torch.cat(\n",
    "            [timesteps,\n",
    "             torch.ones((1, 1), device=device, dtype=torch.long) * (t+1)], dim=1)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    data = {\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'target_return': torch.full_like(target_return, episode_return/scale), # actual return \n",
    "        'timesteps': timesteps,\n",
    "    }\n",
    "\n",
    "    return episode_return, episode_length, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarRacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4\n",
      "Reward 873.5612167300249\n",
      "Track generation: 1115..1398 -> 283-tiles track\n"
     ]
    }
   ],
   "source": [
    "episode_return, episode_length, data = evaluate_episode_rtg(\n",
    "    env, 64, 2, gcsl_predictor, encoder=encoder, target_return=275/100., mode='delayed', scale=100., device='cpu',\n",
    "    state_mean=state_mean_cp, state_std=state_std_cp,\n",
    "    # state_mean=np.zeros_like(state_mean_cp), state_std=np.ones_like(state_std_cp),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.2000000000003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardEngine(nn.Module):\n",
    "    def __init__(self, grid_size=10, dim=64):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.dim = dim\n",
    "        args = dotdict({\n",
    "            'device': 'cuda',\n",
    "            'K': 20,\n",
    "        })\n",
    "        self.gcsl_predictor = GCSL_IDM(dim, 2, 1000, args)\n",
    "        self.gcsl_predictor.load_state_dict(torch.load('tmp\\\\carracing\\\\idm_gcsl_gcsl+\\\\2023_05_12_16_29_53_291165\\\\tensorboard\\\\iter_100.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "    def get_reward(self, state, next_state, dones):\n",
    "        state = torch.tensor(state, dtype=torch.float32)/10 # important to normalize\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)# important to not normalize or make sure is same as preds    \n",
    "        predicted_state = self.state_predictor(state)\n",
    "        pred_x = predicted_state[:,:self.grid_size].argmax(dim=1)\n",
    "        pred_y = predicted_state[:, self.grid_size:].argmax(dim=1)\n",
    "        pred_state = torch.stack((pred_x, pred_y), dim=1).float()\n",
    "        next_state = next_state[:, :2]\n",
    "        # print(pred_state, next_state[:, :2]) \n",
    "        reward = F.mse_loss(pred_state, next_state, reduction='none').sum(dim=1)\n",
    "        reward[dones] = 0 # don't penalize for terminal states\n",
    "\n",
    "        return -1 * reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_drive",
   "language": "python",
   "name": "rl_drive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
